{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a33b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55ce762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b84eb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://medium.com/@aleixlopez/introduction-to-ai-agents-62a790d0bc22', 'title': 'Introduction to AI Agents. Architecture, Tools, and Implementation | by Aleix López Pascual | Medium', 'description': 'Discover the power of AI agents and how they extend the capabilities of Large Language Models (LLMs). Learn about their architecture, core components, and real-world applications in this comprehensive guide.', 'language': 'en'}, page_content='Introduction to AI Agents. Architecture, Tools, and Implementation | by Aleix López Pascual | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inIntroduction to AI AgentsArchitecture, Tools, and ImplementationAleix López Pascual10 min read·Jan 12, 2025--3ListenShareArtificial intelligence continues to reshape the way we live and work. In the past years, Large Language Models (LLMs), such as ChatGPT, have captivated the world with their ability to understand and generate human-like text. They are masters of language, capable of holding conversations, answering complex questions, and even writing code. But beneath their linguistic brilliance lies a fundamental limitation — they lack true autonomy and are limited by their training data. This is where the concept of an agent comes into play. Agents are programs that extend the capabilities of LLMs, enabling them to observe, reason, and act autonomously, using a variety of tools.In this article, we will explore the world of AI agents, covering its architecture, core components, and implementation for real-world applications.For a quick refresher on the basics of LLMs, consider visiting this article:Introduction to Foundational Large Language ModelsThe underlying architecture, training methodologies, fine-tuning techniques, and inference optimization methods for…medium.comWhat is an Agent?In its most basic form, a Generative AI agent is an application that tries to achieve a goal by observing the world and acting upon it using the tools available to it. These agents are autonomous, meaning they can operate independently of human intervention, especially when provided with clear objectives.In contrast to LLMs, which are reactive and require constant prompts from users, AI agents are proactive. LLMs operate within a question-and-answer loop, waiting for input before responding. Despite their impressive capabilities, LLMs remain passive, unable to act without explicit direction.This is where AI agents differentiate themselves. Unlike LLMs, which are limited to responding, AI agents go beyond mere understanding — they take action. They are equipped to not only make decisions but also carry out tasks autonomously. For instance, while an LLM might help you brainstorm a travel itinerary, an AI agent will go a step further: booking your flights, comparing hotel prices, and scheduling your transportation without needing explicit commands for each step.Core Components of an AgentAt its core, an agent’s cognitive architecture comprises three essential components:The Model: This is the language model (LM) that acts as the central decision-maker for the agent. But it cannot be any LM, it has to be capable of following instruction based reasoning and logic frameworks, like ReAct, Chain-of-Thought, or Tree-of-Thoughts. For optimal results, the chosen model should align with the desired application and ideally be trained on data signatures associated with the tools the agent will use. In other words, the model should be familiar with the format, structure, or context of the data it will process with the tools.The Tools: These bridge the gap between the model’s internal capabilities and the external world. Tools enable agents to interact with external data and services, broadening their range of actions. Tools can take various forms such as Extensions, Functions, and Data Stores. Examples include updating a database, fetching weather data, or sending an email.The Orchestration Layer: This layer governs how the agent processes information, performs reasoning, and decides on its next action. It is a cyclical process that continues until the agent reaches its goal or a stopping point. The complexity of this layer can vary significantly, from simple calculations to chained logic and machine learning algorithms. It also includes prompt engineering and associated frameworks to guide reasoning and planning.Press enter or click to view image in full sizeGeneral agent architecture and componentsCognitive Architectures: How Agents OperateCognitive architectures function as the brain of the agent. They are the underlying structures that allow agents to not just process information, but to also reason, make decisions, and refine their actions iteratively to reach a specific goal.Here’s a breakdown of how these architectures function:Cyclical Process: Agents operate in a cyclical fashion, continually taking in information, performing internal reasoning, and using that reasoning to decide on their next action. This cycle continues until the agent achieves its objective or reaches a defined stopping point.Orchestration Layer as the Core: At the heart of any cognitive architecture is the orchestration layer. This layer is responsible for maintaining the agent’s memory, current state, reasoning processes and overall planning.Reasoning Frameworks: The orchestration layer uses prompt engineering and specific frameworks to guide reasoning and planning. These frameworks help the agent to interact with its environment more effectively and complete tasks.These are the most popular reasoning frameworks at the time of writing this article:Reason & Act (ReAct)Chain-of-Thought (CoT)Tree-of-thoughts (ToT)Agents can utilize any of the above reasoning techniques. Basically, what these frameworks will do is to force the LM to think step by step, carefully considering the available information and taking the most appropriate actions based on that reasoning.For a quick refresher on these reasoning frameworks (prompt techniques), consider visiting this article:Introduction to Prompt EngineeringMastering the Art of Prompt Engineering: Techniques, Best Practices, and Real-World Applicationsmedium.comPress enter or click to view image in full sizeAgent with ReAct reasoning in the orchestration layerTools: An Agent’s Connection to the Outside WorldAs we have seen, LMs excel at processing information, but they are inherently limited by their inability to interact with the real world. Tools bridge this critical gap, empowering agents to interact with external data and services, enabling a wider range of actions beyond the capabilities of the model alone.There are three primary tool types for agents: Extensions, Functions and Data Stores.Extensions: Standardized API InteractionsExtensions serve as a standardized bridge between an API and an agent, allowing the agent to execute APIs regardless of their underlying implementation. Think of extensions as pre-built connectors that allow an agent to easily interact with different APIs.How extensions work:They teach the agent how to use an API endpoint through examples.They teach the agent what arguments or parameters are needed to successfully call the API endpoint.The agent uses what it has learnt to decide which Extension, if any, would be suitable for solving the user’s query.Press enter or click to view image in full sizeExtensions connect Agents to External APIsFunctions: Client-Side ControlFunctions are self-contained modules of code that accomplish specific tasks and can be reused as needed, similar to how software developers use functions. In the context of agents, a model decides when to use each function and what arguments it needs based on its specification. The key difference between functions and extensions is that functions are executed on the client-side, whereas extensions are executed on the agent-side.How functions work:The AI agent processes the user’s request and determines that a specific function should be called.The model generates the name of the function to call and the required arguments. Example: If the user asks, “What’s the weather in New York?”, the agent might output:Function: get_weather. Arguments: {“location”: “New York”}The AI agent doesn’t directly make the API call or execute the function itself. It only “recommends” the action (function and arguments) to the surrounding system.The client-side application (the environment where the agent operates) is responsible for interpreting the model’s output and actually calling the function or API.Reasons to use functions:Simplicity: The agent doesn’t need to understand technical aspects of API calls, such as handling authentication tokens, error responses, or networking.Security: Offloading API calls to the client-side reduces the risk of exposing sensitive API keys or mismanaging secure connections.Performance: By not making live API calls, the AI model focuses on reasoning and decision-making, while the client handles real-world interactions.Press enter or click to view image in full sizeHow functions interact with external APIsData Stores: Access to Dynamic InformationData stores address the limitation of language models having static knowledge by providing access to more dynamic and up-to-date information, ensuring the model’s responses remain relevant. Think of a data store as an external, updatable source of information that an agent can tap into.How data stores work:Developers provide data in its original format (spreadsheets, PDFs, etc.) to the agent.The data is converted into a set of vector embeddings.The embeddings are stored in a vector database.A user query is sent to the same embedding model to generate embeddings for the query.The query embeddings are matched against the vector database content using a matching algorithm.The matched content is retrieved and sent to the agent.The agent formulates a response or action based on the user query and the retrieved content.Press enter or click to view image in full sizeExample of an agent interacting with data storesTargeted Learning Approaches to Enhance Model PerformanceTargeted learning approaches focus on training or guiding AI Agents to make better decisions about which tools or resources to use in various situations. There are several approaches to achieve this:In-context learning: The model learns on the fly using prompts, tools, and few-shot examples at inference time. By presenting the model with carefully crafted prompts, including examples on when and how to use the tools, the model can understand the context and determine how to proceed.Retrieval-based in-context learning: This method enhances in-context learning by dynamically retrieving relevant information, examples, or tools from an external memory or database. The retrieved content is then included in the prompt during inference.Fine-tuning based learning: This involves training a model on a specific dataset that includes labeled examples of tool usage, decision-making processes, or reasoning steps. This updates the model’s weights, embedding the knowledge into the model itself. Unlike in-context learning, which relies on prompts, fine-tuning creates a permanent adaptation of the model.Use case: Building an Agent with LangChainIn order to provide a real-world example of an agent in action, we will build a quick prototype using the LangChain framework.LangChain is an open-source framework that simplifies building AI agents by providing modular components for chaining logic, managing memory, and integrating external tools like APIs or databases. One of the benefits of LangChain is that it eliminates the need for verbose prompts to guide the model. The framework inherently provides reasoning instructions to the model and integrates efficiently with external tools, reducing the need for lengthy, detailed prompts to guide behavior.In the provided example, we will use the gemini-1.5-flash-001 model and two tools: SerpAPI (for Google Search) and the Google Places API (for location data).import osfrom langgraph.prebuilt import create_react_agentfrom langchain_core.tools import toolfrom langchain_community.utilities import SerpAPIWrapperfrom langchain_community.tools import GooglePlacesTool# Setting up API keysos.environ[\"SERPAPI_API_KEY\"] = \"XXXXX\"os.environ[\"GPLACES_API_KEY\"] = \"XXXXX\"# Define the search tool using SerpAPI@tooldef search(query: str):    \"\"\"Use the SerpAPI to run a Google Search.\"\"\"    search = SerpAPIWrapper()    return search.run(query)# Define the places tool using Google Places API@tooldef places(query: str):    \"\"\"Use the Google Places API to run a Google Places Query.\"\"\"    places = GooglePlacesTool()    return places.run(query)# Initialize the modelmodel = ChatVertexAI(model=\"gemini-1.5-flash-001\")# List of tools available to the agenttools = [search, places]# Query to ask the agentquery = \"Who did the Texas Longhorns play in football last week? \" \\\\        \"What is the address of the other team\\'s stadium?\"# Create the agent using the model and toolsagent = create_react_agent(model, tools)# Input message structureinput = {\"messages\": [(\"human\", query)]}# Process the agent\\'s response in stream modefor s in agent.stream(input, stream_mode=\"values\"):    message = s[\"messages\"][-1]    if isinstance(message, tuple):        print(message)    else:        message.pretty_print()And this is the output from our agent:=============================== Human Message ==============================Who did the Texas Longhorns play in football last week? What is the address of the other team\\'s stadium? ================================= Ai Message ===============================Tool Calls: search Args:    query: Texas Longhorns football schedule================================= Tool Message =============================Name: search{...Results: \"NCAA Division I Football, Georgia, Date...\"}================================= Ai Message ===============================The Texas Longhorns played the Georgia Bulldogs last week.Tool Calls: places Args:   query: Georgia Bulldogs stadium ================================ Tool Message ==============================Name: places{...Sanford Stadium Address: 100 Sanford...} ================================= Ai Message ===============================The address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA 30602, USA.ConclusionGenerative AI agents are a powerful extension of language models, enabling them to interact with the real world through tools, reasoning, and orchestration. In my opinion, agents are quite exciting as they represent a shift towards more interactive and intelligent systems capable of performing tasks autonomously.Key takeaways include:Agents extend the capabilities of language models by leveraging tools to access real-time information, and execute specific tasks.The brain of the agent lies in the orchestration layer. This layer is responsible for maintaining the agent’s memory, current state, reasoning processes and overall planning.Tools such as Extensions, Functions and Data Stores provide agents with the capacity to interact with external systems and access knowledge beyond their training data.Targeted learning approaches focus on training or guiding AI Agents to make better decisions about which tools or resources to use in various situations.Frameworks like LangChain make building agents much simpler.References[1] Wiesinger, J., Marlow, P., & Vuskovic, V. (2024). Agents.Press enter or click to view image in full sizeAnd that’s it! Thank you for reading! I hope you enjoyed this article. If you’d like, add me on LinkedIn.LlmAgentsAIToolsLangchain----3Written by Aleix López Pascual124 followers·6 followingSenior Data Scientist @ Glovo | Competitions Expert @ Kaggle | Writer @ Medium | MSc in High Energy Physics, Astrophysics and CosmologyResponses (3)See all responsesHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://medium.com/@aleixlopez/introduction-to-embeddings-vector-stores-c04fe3d11953', 'title': 'Introduction to Embeddings & Vector Stores | by Aleix López Pascual | Medium', 'description': 'Discover the power of embeddings in machine learning for processing diverse data types. Learn about embedding techniques, vector databases, vector search, and real-world applications.', 'language': 'en'}, page_content='Introduction to Embeddings & Vector Stores | by Aleix López Pascual | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inIntroduction to Embeddings & Vector StoresThe role of embeddings in machine learning, how to create them, how to manage manage them at scale, and real-world applicationsAleix López Pascual12 min read·Dec 3, 2024--1ListenShareWith the rise of Large Language Models (LLMs), the ability to effectively process and analyze diverse data types (images, video, text, audio, and more) has become increasingly critical. Embeddings have emerged as a cornerstone of modern machine learning, offering a powerful solution to this challenge by transforming heterogeneous data into a unified format suitable for a wide array of applications. In this article, we will explore the fascinating world of embeddings, covering various embedding techniques, how to manage and query embeddings at scale, and real-world applications.What are Embeddings?Embeddings are low-dimensional numerical vector representations of real-world data like text, images, audio, and video. They provide:Compact representation: Embeddings act as a means of lossy compression of the original data while retaining its essential properties, facilitating efficient large-scale data processing and storage.Semantic meaning: The geometric distances between two vectors in the embedding space reflect the relationships between the real-world objects they represent. This allows for comparing the similarity or difference between data objects of different types on a numerical scale.Imagine compressing a complex image or a lengthy document into a concise numerical vector that retains its core meaning — this is the magic of embeddings. By projecting diverse data types into a common vector space, multimodal embeddings can be directly used as input for machine learning models, providing a powerful representation of the data.Press enter or click to view image in full sizeProjecting objects/content into a joint vector space with semantic meaning. Intra-modality refers to relationships or interactions within the same modality. Inter-modality refers to relationships or interactions between different modalities.Types of EmbeddingsLet’s explore some standard embedding techniques for different data types:1. Text EmbeddingsWord Embeddings:Word Embeddings capture the semantic meaning of individual words by leveraging the principle that a word’s meaning is defined by its neighbors in a corpus. Words with similar meanings should have similar embeddings. For instance, the word “computer” should have an embedding that is similar to the embeddings of words like “laptop” and “PC”, but different from the embedding of a word like “car”.These are some of the most popular algorithms:Word2Vec: This algorithm can be further divided into two different techniques:- The Continuous bag of words (CBOW) approach: Tries to predict the middle word, using the embeddings of the surrounding words as input. This method is agnostic to the order of the surrounding words in the context. This approach is fast to train and is slightly more accurate for frequent words.- The skip-gram approach: The setup is inverse of that of CBOW, with the middle word being used to predict the surrounding words within a certain range. This approach is slower to train but works well with small data and is more accurate for rare words.GloVe (Global Vectors for Word Representation): Unlike Word2Vec, Glove not only captures local statistics (close words) but it also captures the global statistics (words in the whole corpus). It does this by first creating a co-occurrence matrix, which represents the relationships between words. Then it uses a factorization technique to learn word representations from the co-occurrence matrix.SWIVEL (Skip-Window Vectors with Negative Sampling): It uses a co-occurrence matrix like GloVe but introduces optimizations that make the training process more scalable and efficient. However, it is slightly less accurate than GloVe.Press enter or click to view image in full size2D visualization of pre-trained GloVe and Word2Vec word embeddingsDocument Embeddings:Document Embeddings represent entire documents or paragraphs as vectors, capturing the meaning of a sequence of words.The early methods of document embeddings relied on the Bag-of-Words (BoW) paradigm. These models treat a document as an unordered collection of words and do not consider the order of words or the relationships between them. Some of the popular algorithms include:Latent Semantic Analysis (LSA): This technique analyses the co-occurrence of words in documents to create a lower-dimensional representation of the document’s meaning.Latent Dirichlet Allocation (LDA): LDA is a probabilistic model that represents documents as mixtures of topics, where each topic is a distribution over words.TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF is a statistical measure that reflects the importance of a word in a document relative to a collection of documents. In other words, it evaluates words based on how often they appear in the document.Doc2Vec: This model, inspired by Word2Vec, attempts to address the limitations of traditional BoW: both the word ordering and the semantic meanings are ignored. To fix this, Doc2Vec adds a “paragraph embedding” to the Word2Vec model. This embedding is trained along with word embeddings to predict words within the document, allowing it to capture some contextual information.In contrast, the latest approaches for document embeddings utilize pre-trained language models, which harness the power of transformers and large-scale datasets. Some of the most popular algorithms include:BERT (Bidirectional Encoder Representations from Transformers): BERT was a groundbreaking model that achieved state-of-the-art results on various NLP tasks in 2018. It is an encoder-only transformer model that utilizes a masked language modeling objective during pre-training, which involves randomly masking some words in the input and training the model to predict the masked words based on the surrounding context. This allows BERT to learn bidirectional representations of words.RoBERTa (Robustly Optimized BERT Pre-Training Approach): RoBERTa is an improved variant of the original BERT model. It enhances BERT’s performance by making several optimizations during pre-training, including more data and training times.Sentence-BERT: Sentence-BERT is specifically designed for sentence embedding tasks. It builds upon BERT’s architecture and is fine-tuned to produce semantically meaningful sentence embeddings.SimCSE (Simple Contrastive Learning of Sentence Embeddings): SimCSE utilizes a contrastive learning approach to train sentence embeddings. It generates positive and negative example pairs from the input sentences and trains the model to maximize the similarity between positive pairs while minimizing the similarity between negative pairs.T5 (Text-to-Text Transfer Transformer): T5 is a large language model that frames all NLP tasks as text-to-text problems. T5 uses an encoder-decoder architecture and it is known for being very versatile in handling NLP tasks.Can Gemini, GPT, Llama or other decoder-only architectures be used for generating document embeddings?In general, generative, auto-regressive models aren’t well suited for embeddings because their understanding of the input is spread out over multiple hidden states. It is better to use a model that has been trained with the specific purpose of producing embeddings. Typically this is a transformer encoder, and in such cases you take the hidden state from the last layer of the last “end of sequence” token. This means that the model’s understanding is concentrated in a single place. More details can be found in this paper:arXiv:2201.10005v1 [cs.CL] 24 Jan 20222. Image embeddingsOne of the most popular methods to derive image embeddings is by training a CNN or Vision Transformer model on a large scale image classification task (for example, ImageNet), and then using the penultimate layer as the image embedding. If successful, this layer will have learnt some important discriminative feature maps. These feature maps represent learned visual patterns that are crucial for image classification or other image tasks.3. Multimodal EmbeddingsTo obtain multimodal embeddings, individual unimodal embeddings (e.g., text embeddings and image embeddings) are combined, taking into account the semantic relationships between the modalities. These relationships are typically learned via a separate training process that aligns the embeddings into a shared latent space.This process ensures that data from different modalities (text, images, audio, video, etc.) are represented in a fixed-size semantic vector in the same latent space, enabling direct comparison and integration. The training often involves techniques such as contrastive learning, where paired data points (e.g., an image and its associated caption) are brought closer in the embedding space, while non-matching pairs are pushed apart. This alignment captures both intra-modality (e.g., relationships within text or within images) and inter-modality (e.g., relationships between text and images) semantics.4. Structured Data EmbeddingsWhen dealing with structured data — that is, data organized in rows and columns — it is necessary to create a custom embedding model tailored to its specific application. In other words, in these cases, it is rare to find a pre-trained embedding model that can be directly used. We can find two common cases:General Structured Data: Embeddings for structured data like sensor data or tabular data can be generated using dimensionality reduction techniques like PCA. These embeddings can be used for anomaly detection and as inputs for downstream machine learning tasks.User/Item Structured Data: This type of structured data goes beyond a general data table. It includes multiple interconnected components: user data (e.g., demographics, preferences, or behaviors), item/product data (e.g., attributes like category, price, or brand), and data that describes the interaction between users and items (e.g., ratings, clicks, or purchase history).The primary goal of working with such data is to map both users and items into a shared embedding space, enabling recommender systems to predict interactions effectively. By representing users and items as fixed-size embeddings, the system can measure their similarity or relevance, making it possible to recommend the most suitable items for each user. One popular technique for this is Collaborative Filtering.5. Graph Embeddings:Graph embeddings are a way to represent nodes, edges, or entire graphs as fixed-size numerical vectors while preserving the structural and relational information inherent in the graph. These embeddings capture both the local (neighborhood) and global (overall structure) properties of the graph, enabling various downstream tasks like node classification, clustering, link prediction, recommendation systems, and more.Take an example of a social network where each person is a node, and the connections between people are defined as edges. Using graph embedding you can model each node as an embedding, such that the embedding captures not only the semantic information about the person itself, but also its relations and associations hence enriching the embedding. For example, if two nodes are connected by an edge, the vectors for those nodes would be similar. You might then be able to predict who the person is most similar to and recommend new connections.Popular algorithms for graph embedding include DeepWalk, Node2vec, LINE, and GraphSAGE.Training EmbeddingsModern embedding models often employ a dual encoder architecture, also known as a two-tower architecture, for training. In this setup, two separate neural networks, or “towers,” are used to encode different aspects of the data. For example, for the text embedding model used in question-answering, one tower is used to encode the queries and the other tower is used to encode the documents. For the image and text embedding model, one tower is used to encode the images and the other tower is used to encode the text.The training process typically involves a contrastive loss, a type of loss function that encourages the model to bring positive examples (semantically similar pairs) closer together in the embedding space while pushing negative examples (dissimilar pairs) further apart.Similar to LLMs training, the training of an embedding model from scratch includes two stages:Pre-training: In this unsupervised phase, the model learns general representations from a massive amount of unlabeled data.Fine-tuning: This supervised phase involves training the model on a smaller, labeled dataset specific to the downstream task. Fine-tuning helps adapt the model’s representations to the specific nuances of the target application.Remember, for unstructured data, pre-trained embeddings are typically available and can be directly used. But you may want to fine-tune them for domain-specific applications.Vector databasesOnce embeddings are created, the next challenge lies in efficiently storing and retrieving them. Historically, traditional databases lacked the means to combine semantic meaning and efficient querying in a way that the most relevant embeddings can be both stored, queried, and retrieved in a secure, scalable, and flexible manner. This is what gave rise to vector databases: specialized systems designed to manage and query embeddings at scale.Each vector database differs in its implementation, but the general flow is shown in the following figure:Press enter or click to view image in full sizeAn appropriate trained embedding model is used to embed the relevant data points as vectors with fixed dimensions.The vectors are then augmented with appropriate metadata and complementary information (such as tags) and indexed using the specified algorithm for efficient search.An incoming query gets embedded with the same model, and used to query and return specific amounts of the most semantically similar items and their associated unembedded content/metadata. Some databases might provide caching and pre-filtering (based on tags) and post-filtering capabilities (reranking using another more accurate model) to further enhance the query speed and performance.A few good examples of commercially managed vector databases areGoogle Vertex AI Vector SearchPineconeAnd some open-source examples are:WeaviateChromaVector searchVector search is the lifeblood of vector databases. It’s the mechanism that allows these databases to unlock the power of semantic similarity and deliver highly relevant results.Traditional vector search algorithms relied on keyword matching, which made them slow, difficult to scale, and incapable of returning results that did not exactly match the query’s keywords. In contrast, modern vector search techniques leverage embeddings to retrieve information based on semantic similarity, using metrics such as Euclidean distance, cosine similarity, or dot product. In other words, vector search methods identify the items in the database that are most similar to the given query based on their vector representations.Approximate Nearest Neighbor SearchOne of the most popular techniques for vector search is Approximate Nearest Neighbor (ANN) Search. ANN refers to a collection of algorithms designed to efficiently identify the closest (most similar) vectors in high-dimensional spaces.Why ANN?In vector databases (high-dimensional spaces), the computational cost of exact nearest neighbor (NN) searches grows exponentially with the size of the dataset and the number of dimensions. Instead, what ANN algorithms do is to trade off exact accuracy for speed and scalability, delivering results that are “good enough” in a fraction of the time.Popular ANN algorithms include:1. Hierarchical Navigable Small Worlds (HNSW)Strengths:- Extremely fast with high accuracy.- Scalable for large datasets.- Flexible in adapting to different similarity measures.Use Cases: General-purpose ANN tasks, recommendation systems, and high-dimensional vector retrieval.Tools: Integrated into libraries like FAISS and nmslib.2. Scalable Approximate Nearest Neighbor (ScaNN)Strengths:- Optimized for industrial-scale datasets.- Combines partitioning, approximate distance computation, and optional rescoring for top-tier speed-accuracy trade-offs.- Handles both dense and sparse vectors effectively.Use Cases: Google-scale applications, such as search and retrieval for embeddings.Tools: ScaNN.3. Annoy (Approximate Nearest Neighbors)Strengths:- Lightweight and simple to use.- Optimized for memory-mapped storage, making it great for static datasets.Use Cases: Recommendation systems, especially for smaller datasets or use cases where memory efficiency is key.Tools: Annoy.4. FAISS (Facebook AI Similarity Search)Strengths:- Comprehensive library supporting multiple ANN algorithms, including HNSW, IVFPQ (Inverted File Index with Product Quantization), and LSH.- Highly optimized for both CPU and GPU, making it ideal for large-scale searches.Use Cases: Large datasets with billions of vectors, real-time recommendations.Tools: FAISS.5. Locality Sensitive Hashing (LSH)Strengths:- Excellent for very high-dimensional sparse data.- Simple to implement and well-understood.Use Cases: Text search, genomic data, or any use case requiring radius-based similarity search.Tools: Available in libraries like scikit-learn and datasketch.Use case: Building a Knowledge-Intensive LLM with RAG and Vector DatabasesImagine you’re building a customer support chatbot that needs to provide accurate and up-to-date answers to customer queries. To achieve this, you can leverage a combination of LLMs, Retrieval Augmented Generation (RAG), and vector databases.Here’s a breakdown of the process:1. Data Preparation:Document Collection: Gather relevant documents like FAQs, product manuals, and customer support logs.Vectorization: Use a language model (e.g., BERT, RoBERTa) to convert each document into a dense vector representation. This vector captures the semantic meaning of the text.2. Vector Database:Indexing: Store these vectors in a vector database (e.g., Pinecone, Weaviate, Chroma). This database is optimized for efficient similarity search.3. RAG System:Query Processing: When a customer asks a question, convert it into a vector representation using the same language model.Retrieval: Query the vector database to find the most similar documents to the query vector.Augmentation: Combine the retrieved documents with the original query and feed them into the LLM.4. LLM Response Generation:The LLM processes the augmented prompt and generates a comprehensive and informative response.The response can be a direct answer to the query, a summary of relevant information, or a combination of both.ConclusionEmbeddings have revolutionized the way we work with diverse data in machine learning. Their ability to capture semantic relationships, facilitate efficient comparisons, and serve as inputs for various models has opened up a world of possibilities. By choosing the right embedding technique, vector database, and search algorithm, developers can unlock the full potential of embeddings to build powerful applications.Key use cases, such as semantic search, recommendation systems, anomaly detection, and LLMs with RAG, showcase the versatility of embeddings in addressing complex challenges. As the technology continues to evolve, embeddings will remain a fundamental building block for advancing AI-driven solutions.If you’re interested in related content, I recommend reading my next article, Introduction to AI Agents. It explores the world of AI agents, covering its architecture, core components, and implementation for real-world applications.Introduction to AI AgentsArchitecture, Tools, and Implementationmedium.comReferences[1] Nawalgaria, A., & Ren, X. (2024). Embeddings & Vector Stores.Press enter or click to view image in full sizeAnd that’s it! Thank you for reading! I hope you enjoyed this article. If you’d like, add me on LinkedIn.EmbeddingVector DatabaseVector SearchLlmAI----1Written by Aleix López Pascual124 followers·6 followingSenior Data Scientist @ Glovo | Competitions Expert @ Kaggle | Writer @ Medium | MSc in High Energy Physics, Astrophysics and CosmologyResponses (1)See all responsesHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://medium.com/@aleixlopez/introduction-to-prompt-engineering-16fb3cbfb031', 'title': 'Introduction to Prompt Engineering | by Aleix López Pascual | Medium', 'description': 'Learn the fundamentals of prompt engineering and unlock the full potential of Large Language Models (LLMs). Discover essential techniques, best practices, and practical examples to master this critical AI skill.', 'language': 'en'}, page_content='Introduction to Prompt Engineering | by Aleix López Pascual | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inIntroduction to Prompt EngineeringMastering the Art of Prompt Engineering: Techniques, Best Practices, and Real-World ApplicationsAleix López Pascual10 min read·Nov 23, 2024--ListenShareThis article explores the fascinating world of prompt engineering, a critical skill for effectively leveraging the power of Large Language Models (LLMs) in a variety of applications. We’ll cover essential techniques, best practices, and practical examples to help you become a proficient prompt engineer.For a quick refresher on the basics of LLMs, consider visiting this article:Introduction to Foundational Large Language ModelsThe underlying architecture, training methodologies, fine-tuning techniques, and inference optimization methods for…medium.comUnderstanding Prompts and Prompt EngineeringAt its core, a prompt is the input you provide to an LLM to elicit a desired response. It could be a question, a statement, instructions, or even a combination of text and other modalities like images. Prompt engineering, then, is the art and science of crafting effective prompts to guide LLMs towards generating accurate and insightful outputs.It’s an iterative process involving experimentation, optimisation, and careful consideration of various factors that influence LLM behaviour.Controlling LLM Output: Configuration is KeyLLMs offer a range of configuration settings that significantly impact the generated text. Understanding these settings is crucial for tailoring the output to your specific needs.Output Length: Determines the maximum number of tokens the LLM will generate. Generating more tokens requires more computation from the LLM, leading to higher energy consumption, potentially slower response times, and higher costs.Temperature: Controls the randomness of the LLM’s output. Lower temperature values (e.g., 0.1) make the responses more predictable and deterministic, returning the tokens with highest probability. Higher temperatures (e.g., 0.9) introduce more randomness, allowing for more diverse and creative results.Top-K Sampling: Imagine the LLM has generated a probability distribution for the next token in a sequence. Top-K sampling restricts the LLM to consider only the K most likely tokens from this distribution. For instance, if K = 5, the LLM will choose the next token randomly from only the top 5 most likely candidates.Top-P Sampling (Nucleus Sampling): This method takes a slightly different approach. Instead of fixing the number of tokens to consider, it sets a probability threshold P. The LLM then selects tokens from the probability distribution, starting with the most likely, until the cumulative probability of the selected tokens reaches or exceeds P. For example, if P = 0.9, the LLM might choose the top 3 most likely tokens if their combined probability is 0.92, even though other less likely tokens exist.Examples:Press enter or click to view image in full sizeIt is recommended starting with a temperature of 0.2, top-P of 0.95, and top-K of 30 for generally coherent and creative results.Prompting TechniquesLet’s delve into a variety of prompting techniques that enable you to interact with LLMs more effectively and achieve your desired results.Zero-Shot Prompting:This is the most basic approach, where you provide a task description and some input text without any examples. It relies on the LLM’s inherent knowledge and ability to generalize from its training data.Press enter or click to view image in full sizeOne-Shot and Few-Shot Prompting:These techniques involve providing one or more examples within the prompt to guide the LLM towards a specific output structure or pattern. The idea is to show the model what you expect, making it particularly helpful for tasks like code generation, translation, or data extraction.Press enter or click to view image in full sizeSystem prompting:Sets the overall context and purpose for the language model. It defines the ‘big picture’ of the model’s intended task, such as language translation, review classification, etc. It also used to return a certain structure or format, for instance JSON format or uppercase.Press enter or click to view image in full sizeContextual prompting:Provides specific details or background information relevant to the current conversation or task. It helps the model comprehend the nuances of the query and tailor the response accordingly.Press enter or click to view image in full sizeRole prompting:Asks the LLM to adopt a specific persona, influencing the tone and style of its output. This helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior.Press enter or click to view image in full sizeStep-Back Prompting:This technique encourages the LLM to think more critically by first prompting it to first consider a general question related to the specific task at hand, and then feeding the answer to that general question into a subsequent prompt for the specific task. This “step back” helps activate relevant background knowledge and can lead to more insightful and accurate responses.Prompt 1:Press enter or click to view image in full sizePrompt 2:Press enter or click to view image in full sizeChain of Thought (CoT) Prompting:CoT enhances the LLM’s reasoning abilities by explicitly prompting it to generate intermediate reasoning steps. This makes the reasoning process transparent, improves accuracy, and can be particularly helpful for complex tasks involving logical deduction or problem-solving.Press enter or click to view image in full sizeSelf-Consistency Prompting:This technique leverages the power of multiple reasoning paths to improve accuracy. It works by sending the same CoT prompt multiple times with a high-temperature setting, encouraging diverse reasoning, and then selecting the most frequent answer. This offers a sort of “wisdom of the crowd” approach to LLM reasoning.Tree of Thoughts (ToT) Prompting:ToT further generalizes CoT prompting by allowing the LLM to explore multiple reasoning paths concurrently, like branches on a tree. Unlike CoT, which follows a single linear chain of thought, ToT enables exploration of various possibilities, making it particularly effective for complex tasks that require exploring various possibilities before arriving at a solution.Implementation: ToT can be implemented manually. This involves crafting specific prompts that guide the LLM to explore multiple reasoning paths. Then, continue prompting in the most promising direction to continue exploring. If a prompt is unproductive, we backtrack to a previous prompt and explore alternative paths.Alternatively, it can be implemented automatically using a Python script or libraries like LangChain.ReAct (Reason & Act) Prompting:ReAct is a popular technique that enhances the capabilities of LLMs by enabling them to interact with external tools and APIs. This interaction allows the LLM to retrieve information, perform computations, and execute actions to solve complex problems that would be difficult or impossible with text-based input alone.Here’s a breakdown of how ReAct prompting works:Thought-Action Loop: ReAct prompting operates through a continuous loop of reasoning and acting. The LLM starts by analysing the given prompt and formulates a plan of action. This plan may involve utilising external tools to gather additional information.Reasoning and Planning: The LLM uses its internal knowledge and the information gathered from external sources to reason about the problem. It breaks down the problem into smaller steps and devises a strategy to achieve the desired outcome.Action Execution: The LLM then interacts with the appropriate external tools, such as search engines, APIs, or databases, to execute the planned actions. This could involve searching for relevant information, retrieving specific data points, or performing calculations.Observation and Feedback: The LLM observes the results of its actions and incorporates the new information into its reasoning process. This feedback loop allows the LLM to adapt its plan and refine its approach as it progresses towards the solution.Example:Imagine you want to know the total number of children fathered by members of the band Metallica. A ReAct prompt would instruct the LLM to:Identify the members of Metallica.For each member, search for information about their children.Sum the number of children across all band members.The LLM would then interact with a search engine API, retrieving information about each band member and their offspring. By combining reasoning and external tool usage, the LLM can provide an accurate answer to this question.Implementation:Implementing ReAct prompting typically involves using libraries like LangChain, which provides tools for integrating LLMs with external APIs and resources. Developers can then create agents that leverage ReAct prompting to solve specific problems.Automatic Prompt EngineeringIn the previous section we explored various manual prompting techniques, which required careful crafting and refinement by humans. Wouldn’t it be nice to automate this (write a prompt to write prompts)? Well, there’s a method: Automatic Prompt Engineering (APE).The core idea behind APE is to use an LLM to generate a variety of prompt candidates, evaluate these candidates based on a chosen metric, and then select the best-performing prompt. This iterative process can be repeated to further refine the prompt and optimise its effectiveness.Here is a breakdown of the steps involved in APE:Prompt Generation: An initial prompt is given to the LLM with instructions to generate multiple variations of the prompt, while maintaining the same semantic meaning. For instance, if the goal is to train a chatbot for a merchandise t-shirt webshop, the initial prompt could ask the LLM to generate different ways a customer might order a specific t-shirt.Prompt Evaluation: Each of the generated prompt candidates is then evaluated using a suitable metric. Common metrics used for evaluating prompts include BLEU (Bilingual Evaluation Understudy) and ROUGE (Recall-Oriented Understudy for Gisting Evaluation). These metrics assess the quality and relevance of the generated prompts based on various criteria, such as fluency, coherence, and semantic similarity to the original prompt.Prompt Selection: The prompt candidate with the highest evaluation score is selected as the final prompt for the intended application. This selected prompt can be further tweaked and re-evaluated if needed.Example:Let’s consider the t-shirt webshop chatbot again. You provide an initial prompt to an LLM, such as: “We have a band merchandise t-shirt webshop, and to train a chatbot, we need various ways to order: ‘One Metallica t-shirt size S’. Generate 10 variants with the same semantics but keep the same meaning”. The LLM would then generate a list of alternative ways to phrase this order.Best Practices for Effective Prompt EngineeringTo truly master the art of prompt engineering, here are some best practices distilled from the sources:Provide Examples: Whenever possible, use one-shot or few-shot learning to demonstrate the desired output structure or pattern. Examples act as powerful teaching tools for LLMs.Design with Simplicity: Keep your prompts concise, clear, and easy to understand. Avoid complex language and unnecessary information. Use verbs that clearly describe the desired action (e.g., summarise, explain, generate).Be Specific About the Output: Don’t be vague! Provide explicit instructions on the desired format, style, length, and content of the response.Prioritise Instructions Over Constraints: Focus on telling the LLM what to do, rather than what not to do. Use constraints only when necessary for safety, clarity, or specific requirements.Control Max Token Length: Set limits either in the configuration or explicitly in the prompt to manage output length and cost.Use Variables in Prompts: Enhance reusability and flexibility by incorporating variables that can be easily changed for different inputs. This is particularly useful for building applications that interact with LLMs.Experiment and Iterate: The key to success in prompt engineering is experimentation. Try different input formats, writing styles, model configurations, and even collaborate with other prompt engineers to compare approaches and discover what works best for your specific tasks.Adapt to Model Updates: LLMs are constantly evolving. Stay informed about updates to model architecture, training data, and capabilities. Revisit and refine your prompts to leverage new features effectively.Explore Output Formats: Consider requesting structured output formats like JSON for tasks involving data extraction, organisation, or analysis. This can streamline downstream processing and reduce the need for manual formatting.Document Your Experiments: Maintain a detailed record of your prompt attempts, model configurations, outputs, and observations. This will help you track progress, identify patterns, and refine your prompting strategies over time.ConclusionPrompt engineering is a journey of continuous exploration and refinement. The techniques and best practices discussed in this article provide a solid foundation for you to start experimenting, iterating, and ultimately mastering the art of communicating effectively with LLMs. Embrace the iterative nature of prompt engineering, document your findings, and continue to push the boundaries of what’s possible with these powerful language models. As the field of LLMs advances, so too will the importance of prompt engineering in unlocking their full potential.If you’re interested in related content, I recommend reading my next article, Introduction to Embeddings & Vector Stores. It explores the role of embeddings in machine learning, how to create them, how to manage manage them at scale, and real-world applications.Introduction to Embeddings & Vector StoresThe role of embeddings in machine learning, how to create them, how to manage manage them at scale, and real-world…medium.comReferences[1] Boonstra, L. (2024). Prompt Engineering.Press enter or click to view image in full sizeAnd that’s it! Thank you for reading! I hope you enjoyed this article. If you’d like, add me on LinkedIn.Prompt EngineeringLarge Language ModelsArtificial IntelligenceData ScienceMachine Learning----Written by Aleix López Pascual124 followers·6 followingSenior Data Scientist @ Glovo | Competitions Expert @ Kaggle | Writer @ Medium | MSc in High Energy Physics, Astrophysics and CosmologyNo responses yetHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://medium.com/@aleixlopez/introduction-to-ai-agents-62a790d0bc22\",\n",
    "    \"https://medium.com/@aleixlopez/introduction-to-embeddings-vector-stores-c04fe3d11953\",\n",
    "    \"https://medium.com/@aleixlopez/introduction-to-prompt-engineering-16fb3cbfb031\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2edbb09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/@aleixlopez/introduction-to-ai-agents-62a790d0bc22', 'title': 'Introduction to AI Agents. Architecture, Tools, and Implementation | by Aleix López Pascual | Medium', 'description': 'Discover the power of AI agents and how they extend the capabilities of Large Language Models (LLMs). Learn about their architecture, core components, and real-world applications in this comprehensive guide.', 'language': 'en'}, page_content='Introduction to AI Agents. Architecture, Tools, and Implementation | by Aleix López Pascual | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inIntroduction to AI AgentsArchitecture, Tools, and ImplementationAleix López Pascual10 min read·Jan 12, 2025--3ListenShareArtificial intelligence continues to reshape the way we live and work. In the past years, Large Language Models (LLMs), such as ChatGPT, have captivated the world with their ability to understand and generate human-like text. They are masters of language, capable of holding conversations, answering complex questions, and even writing code. But beneath their linguistic brilliance lies a fundamental limitation — they lack true autonomy and are limited by their training data. This is where the concept of an agent comes into play. Agents are programs that extend the capabilities of LLMs, enabling them to observe, reason, and act autonomously, using a variety of tools.In this article, we will explore the world of AI agents, covering its architecture, core components, and implementation for real-world applications.For a quick refresher on the basics of LLMs, consider visiting this article:Introduction to Foundational Large Language ModelsThe underlying architecture, training methodologies, fine-tuning techniques, and inference optimization methods for…medium.comWhat is an Agent?In its most basic form, a Generative AI agent is an application that tries to achieve a goal by observing the world and acting upon it using the tools available to it. These agents are autonomous, meaning they can operate independently of human intervention, especially when provided with clear objectives.In contrast to LLMs, which are reactive and require constant prompts from users, AI agents are proactive. LLMs operate within a question-and-answer loop, waiting for input before responding. Despite their impressive capabilities, LLMs remain passive, unable to act without explicit direction.This is where AI agents differentiate themselves. Unlike LLMs, which are limited to responding, AI agents go beyond mere understanding — they take action. They are equipped to not only make decisions but also carry out tasks autonomously. For instance, while an LLM might help you brainstorm a travel itinerary, an AI agent will go a step further: booking your flights, comparing hotel prices, and scheduling your transportation without needing explicit commands for each step.Core Components of an AgentAt its core, an agent’s cognitive architecture comprises three essential components:The Model: This is the language model (LM) that acts as the central decision-maker for the agent. But it cannot be any LM, it has to be capable of following instruction based reasoning and logic frameworks, like ReAct, Chain-of-Thought, or Tree-of-Thoughts. For optimal results, the chosen model should align with the desired application and ideally be trained on data signatures associated with the tools the agent will use. In other words, the model should be familiar with the format, structure, or context of the data it will process with the tools.The Tools: These bridge the gap between the model’s internal capabilities and the external world. Tools enable agents to interact with external data and services, broadening their range of actions. Tools can take various forms such as Extensions, Functions, and Data Stores. Examples include updating a database, fetching weather data, or sending an email.The Orchestration Layer: This layer governs how the agent processes information, performs reasoning, and decides on its next action. It is a cyclical process that continues until the agent reaches its goal or a stopping point. The complexity of this layer can vary significantly, from simple calculations to chained logic and machine learning algorithms. It also includes prompt engineering and associated frameworks to guide reasoning and planning.Press enter or click to view image in full sizeGeneral agent architecture and componentsCognitive Architectures: How Agents OperateCognitive architectures function as the brain of the agent. They are the underlying structures that allow agents to not just process information, but to also reason, make decisions, and refine their actions iteratively to reach a specific goal.Here’s a breakdown of how these architectures function:Cyclical Process: Agents operate in a cyclical fashion, continually taking in information, performing internal reasoning, and using that reasoning to decide on their next action. This cycle continues until the agent achieves its objective or reaches a defined stopping point.Orchestration Layer as the Core: At the heart of any cognitive architecture is the orchestration layer. This layer is responsible for maintaining the agent’s memory, current state, reasoning processes and overall planning.Reasoning Frameworks: The orchestration layer uses prompt engineering and specific frameworks to guide reasoning and planning. These frameworks help the agent to interact with its environment more effectively and complete tasks.These are the most popular reasoning frameworks at the time of writing this article:Reason & Act (ReAct)Chain-of-Thought (CoT)Tree-of-thoughts (ToT)Agents can utilize any of the above reasoning techniques. Basically, what these frameworks will do is to force the LM to think step by step, carefully considering the available information and taking the most appropriate actions based on that reasoning.For a quick refresher on these reasoning frameworks (prompt techniques), consider visiting this article:Introduction to Prompt EngineeringMastering the Art of Prompt Engineering: Techniques, Best Practices, and Real-World Applicationsmedium.comPress enter or click to view image in full sizeAgent with ReAct reasoning in the orchestration layerTools: An Agent’s Connection to the Outside WorldAs we have seen, LMs excel at processing information, but they are inherently limited by their inability to interact with the real world. Tools bridge this critical gap, empowering agents to interact with external data and services, enabling a wider range of actions beyond the capabilities of the model alone.There are three primary tool types for agents: Extensions, Functions and Data Stores.Extensions: Standardized API InteractionsExtensions serve as a standardized bridge between an API and an agent, allowing the agent to execute APIs regardless of their underlying implementation. Think of extensions as pre-built connectors that allow an agent to easily interact with different APIs.How extensions work:They teach the agent how to use an API endpoint through examples.They teach the agent what arguments or parameters are needed to successfully call the API endpoint.The agent uses what it has learnt to decide which Extension, if any, would be suitable for solving the user’s query.Press enter or click to view image in full sizeExtensions connect Agents to External APIsFunctions: Client-Side ControlFunctions are self-contained modules of code that accomplish specific tasks and can be reused as needed, similar to how software developers use functions. In the context of agents, a model decides when to use each function and what arguments it needs based on its specification. The key difference between functions and extensions is that functions are executed on the client-side, whereas extensions are executed on the agent-side.How functions work:The AI agent processes the user’s request and determines that a specific function should be called.The model generates the name of the function to call and the required arguments. Example: If the user asks, “What’s the weather in New York?”, the agent might output:Function: get_weather. Arguments: {“location”: “New York”}The AI agent doesn’t directly make the API call or execute the function itself. It only “recommends” the action (function and arguments) to the surrounding system.The client-side application (the environment where the agent operates) is responsible for interpreting the model’s output and actually calling the function or API.Reasons to use functions:Simplicity: The agent doesn’t need to understand technical aspects of API calls, such as handling authentication tokens, error responses, or networking.Security: Offloading API calls to the client-side reduces the risk of exposing sensitive API keys or mismanaging secure connections.Performance: By not making live API calls, the AI model focuses on reasoning and decision-making, while the client handles real-world interactions.Press enter or click to view image in full sizeHow functions interact with external APIsData Stores: Access to Dynamic InformationData stores address the limitation of language models having static knowledge by providing access to more dynamic and up-to-date information, ensuring the model’s responses remain relevant. Think of a data store as an external, updatable source of information that an agent can tap into.How data stores work:Developers provide data in its original format (spreadsheets, PDFs, etc.) to the agent.The data is converted into a set of vector embeddings.The embeddings are stored in a vector database.A user query is sent to the same embedding model to generate embeddings for the query.The query embeddings are matched against the vector database content using a matching algorithm.The matched content is retrieved and sent to the agent.The agent formulates a response or action based on the user query and the retrieved content.Press enter or click to view image in full sizeExample of an agent interacting with data storesTargeted Learning Approaches to Enhance Model PerformanceTargeted learning approaches focus on training or guiding AI Agents to make better decisions about which tools or resources to use in various situations. There are several approaches to achieve this:In-context learning: The model learns on the fly using prompts, tools, and few-shot examples at inference time. By presenting the model with carefully crafted prompts, including examples on when and how to use the tools, the model can understand the context and determine how to proceed.Retrieval-based in-context learning: This method enhances in-context learning by dynamically retrieving relevant information, examples, or tools from an external memory or database. The retrieved content is then included in the prompt during inference.Fine-tuning based learning: This involves training a model on a specific dataset that includes labeled examples of tool usage, decision-making processes, or reasoning steps. This updates the model’s weights, embedding the knowledge into the model itself. Unlike in-context learning, which relies on prompts, fine-tuning creates a permanent adaptation of the model.Use case: Building an Agent with LangChainIn order to provide a real-world example of an agent in action, we will build a quick prototype using the LangChain framework.LangChain is an open-source framework that simplifies building AI agents by providing modular components for chaining logic, managing memory, and integrating external tools like APIs or databases. One of the benefits of LangChain is that it eliminates the need for verbose prompts to guide the model. The framework inherently provides reasoning instructions to the model and integrates efficiently with external tools, reducing the need for lengthy, detailed prompts to guide behavior.In the provided example, we will use the gemini-1.5-flash-001 model and two tools: SerpAPI (for Google Search) and the Google Places API (for location data).import osfrom langgraph.prebuilt import create_react_agentfrom langchain_core.tools import toolfrom langchain_community.utilities import SerpAPIWrapperfrom langchain_community.tools import GooglePlacesTool# Setting up API keysos.environ[\"SERPAPI_API_KEY\"] = \"XXXXX\"os.environ[\"GPLACES_API_KEY\"] = \"XXXXX\"# Define the search tool using SerpAPI@tooldef search(query: str):    \"\"\"Use the SerpAPI to run a Google Search.\"\"\"    search = SerpAPIWrapper()    return search.run(query)# Define the places tool using Google Places API@tooldef places(query: str):    \"\"\"Use the Google Places API to run a Google Places Query.\"\"\"    places = GooglePlacesTool()    return places.run(query)# Initialize the modelmodel = ChatVertexAI(model=\"gemini-1.5-flash-001\")# List of tools available to the agenttools = [search, places]# Query to ask the agentquery = \"Who did the Texas Longhorns play in football last week? \" \\\\        \"What is the address of the other team\\'s stadium?\"# Create the agent using the model and toolsagent = create_react_agent(model, tools)# Input message structureinput = {\"messages\": [(\"human\", query)]}# Process the agent\\'s response in stream modefor s in agent.stream(input, stream_mode=\"values\"):    message = s[\"messages\"][-1]    if isinstance(message, tuple):        print(message)    else:        message.pretty_print()And this is the output from our agent:=============================== Human Message ==============================Who did the Texas Longhorns play in football last week? What is the address of the other team\\'s stadium? ================================= Ai Message ===============================Tool Calls: search Args:    query: Texas Longhorns football schedule================================= Tool Message =============================Name: search{...Results: \"NCAA Division I Football, Georgia, Date...\"}================================= Ai Message ===============================The Texas Longhorns played the Georgia Bulldogs last week.Tool Calls: places Args:   query: Georgia Bulldogs stadium ================================ Tool Message ==============================Name: places{...Sanford Stadium Address: 100 Sanford...} ================================= Ai Message ===============================The address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA 30602, USA.ConclusionGenerative AI agents are a powerful extension of language models, enabling them to interact with the real world through tools, reasoning, and orchestration. In my opinion, agents are quite exciting as they represent a shift towards more interactive and intelligent systems capable of performing tasks autonomously.Key takeaways include:Agents extend the capabilities of language models by leveraging tools to access real-time information, and execute specific tasks.The brain of the agent lies in the orchestration layer. This layer is responsible for maintaining the agent’s memory, current state, reasoning processes and overall planning.Tools such as Extensions, Functions and Data Stores provide agents with the capacity to interact with external systems and access knowledge beyond their training data.Targeted learning approaches focus on training or guiding AI Agents to make better decisions about which tools or resources to use in various situations.Frameworks like LangChain make building agents much simpler.References[1] Wiesinger, J., Marlow, P., & Vuskovic, V. (2024). Agents.Press enter or click to view image in full sizeAnd that’s it! Thank you for reading! I hope you enjoyed this article. If you’d like, add me on LinkedIn.LlmAgentsAIToolsLangchain----3Written by Aleix López Pascual124 followers·6 followingSenior Data Scientist @ Glovo | Competitions Expert @ Kaggle | Writer @ Medium | MSc in High Energy Physics, Astrophysics and CosmologyResponses (3)See all responsesHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://medium.com/@aleixlopez/introduction-to-embeddings-vector-stores-c04fe3d11953', 'title': 'Introduction to Embeddings & Vector Stores | by Aleix López Pascual | Medium', 'description': 'Discover the power of embeddings in machine learning for processing diverse data types. Learn about embedding techniques, vector databases, vector search, and real-world applications.', 'language': 'en'}, page_content='Introduction to Embeddings & Vector Stores | by Aleix López Pascual | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inIntroduction to Embeddings & Vector StoresThe role of embeddings in machine learning, how to create them, how to manage manage them at scale, and real-world applicationsAleix López Pascual12 min read·Dec 3, 2024--1ListenShareWith the rise of Large Language Models (LLMs), the ability to effectively process and analyze diverse data types (images, video, text, audio, and more) has become increasingly critical. Embeddings have emerged as a cornerstone of modern machine learning, offering a powerful solution to this challenge by transforming heterogeneous data into a unified format suitable for a wide array of applications. In this article, we will explore the fascinating world of embeddings, covering various embedding techniques, how to manage and query embeddings at scale, and real-world applications.What are Embeddings?Embeddings are low-dimensional numerical vector representations of real-world data like text, images, audio, and video. They provide:Compact representation: Embeddings act as a means of lossy compression of the original data while retaining its essential properties, facilitating efficient large-scale data processing and storage.Semantic meaning: The geometric distances between two vectors in the embedding space reflect the relationships between the real-world objects they represent. This allows for comparing the similarity or difference between data objects of different types on a numerical scale.Imagine compressing a complex image or a lengthy document into a concise numerical vector that retains its core meaning — this is the magic of embeddings. By projecting diverse data types into a common vector space, multimodal embeddings can be directly used as input for machine learning models, providing a powerful representation of the data.Press enter or click to view image in full sizeProjecting objects/content into a joint vector space with semantic meaning. Intra-modality refers to relationships or interactions within the same modality. Inter-modality refers to relationships or interactions between different modalities.Types of EmbeddingsLet’s explore some standard embedding techniques for different data types:1. Text EmbeddingsWord Embeddings:Word Embeddings capture the semantic meaning of individual words by leveraging the principle that a word’s meaning is defined by its neighbors in a corpus. Words with similar meanings should have similar embeddings. For instance, the word “computer” should have an embedding that is similar to the embeddings of words like “laptop” and “PC”, but different from the embedding of a word like “car”.These are some of the most popular algorithms:Word2Vec: This algorithm can be further divided into two different techniques:- The Continuous bag of words (CBOW) approach: Tries to predict the middle word, using the embeddings of the surrounding words as input. This method is agnostic to the order of the surrounding words in the context. This approach is fast to train and is slightly more accurate for frequent words.- The skip-gram approach: The setup is inverse of that of CBOW, with the middle word being used to predict the surrounding words within a certain range. This approach is slower to train but works well with small data and is more accurate for rare words.GloVe (Global Vectors for Word Representation): Unlike Word2Vec, Glove not only captures local statistics (close words) but it also captures the global statistics (words in the whole corpus). It does this by first creating a co-occurrence matrix, which represents the relationships between words. Then it uses a factorization technique to learn word representations from the co-occurrence matrix.SWIVEL (Skip-Window Vectors with Negative Sampling): It uses a co-occurrence matrix like GloVe but introduces optimizations that make the training process more scalable and efficient. However, it is slightly less accurate than GloVe.Press enter or click to view image in full size2D visualization of pre-trained GloVe and Word2Vec word embeddingsDocument Embeddings:Document Embeddings represent entire documents or paragraphs as vectors, capturing the meaning of a sequence of words.The early methods of document embeddings relied on the Bag-of-Words (BoW) paradigm. These models treat a document as an unordered collection of words and do not consider the order of words or the relationships between them. Some of the popular algorithms include:Latent Semantic Analysis (LSA): This technique analyses the co-occurrence of words in documents to create a lower-dimensional representation of the document’s meaning.Latent Dirichlet Allocation (LDA): LDA is a probabilistic model that represents documents as mixtures of topics, where each topic is a distribution over words.TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF is a statistical measure that reflects the importance of a word in a document relative to a collection of documents. In other words, it evaluates words based on how often they appear in the document.Doc2Vec: This model, inspired by Word2Vec, attempts to address the limitations of traditional BoW: both the word ordering and the semantic meanings are ignored. To fix this, Doc2Vec adds a “paragraph embedding” to the Word2Vec model. This embedding is trained along with word embeddings to predict words within the document, allowing it to capture some contextual information.In contrast, the latest approaches for document embeddings utilize pre-trained language models, which harness the power of transformers and large-scale datasets. Some of the most popular algorithms include:BERT (Bidirectional Encoder Representations from Transformers): BERT was a groundbreaking model that achieved state-of-the-art results on various NLP tasks in 2018. It is an encoder-only transformer model that utilizes a masked language modeling objective during pre-training, which involves randomly masking some words in the input and training the model to predict the masked words based on the surrounding context. This allows BERT to learn bidirectional representations of words.RoBERTa (Robustly Optimized BERT Pre-Training Approach): RoBERTa is an improved variant of the original BERT model. It enhances BERT’s performance by making several optimizations during pre-training, including more data and training times.Sentence-BERT: Sentence-BERT is specifically designed for sentence embedding tasks. It builds upon BERT’s architecture and is fine-tuned to produce semantically meaningful sentence embeddings.SimCSE (Simple Contrastive Learning of Sentence Embeddings): SimCSE utilizes a contrastive learning approach to train sentence embeddings. It generates positive and negative example pairs from the input sentences and trains the model to maximize the similarity between positive pairs while minimizing the similarity between negative pairs.T5 (Text-to-Text Transfer Transformer): T5 is a large language model that frames all NLP tasks as text-to-text problems. T5 uses an encoder-decoder architecture and it is known for being very versatile in handling NLP tasks.Can Gemini, GPT, Llama or other decoder-only architectures be used for generating document embeddings?In general, generative, auto-regressive models aren’t well suited for embeddings because their understanding of the input is spread out over multiple hidden states. It is better to use a model that has been trained with the specific purpose of producing embeddings. Typically this is a transformer encoder, and in such cases you take the hidden state from the last layer of the last “end of sequence” token. This means that the model’s understanding is concentrated in a single place. More details can be found in this paper:arXiv:2201.10005v1 [cs.CL] 24 Jan 20222. Image embeddingsOne of the most popular methods to derive image embeddings is by training a CNN or Vision Transformer model on a large scale image classification task (for example, ImageNet), and then using the penultimate layer as the image embedding. If successful, this layer will have learnt some important discriminative feature maps. These feature maps represent learned visual patterns that are crucial for image classification or other image tasks.3. Multimodal EmbeddingsTo obtain multimodal embeddings, individual unimodal embeddings (e.g., text embeddings and image embeddings) are combined, taking into account the semantic relationships between the modalities. These relationships are typically learned via a separate training process that aligns the embeddings into a shared latent space.This process ensures that data from different modalities (text, images, audio, video, etc.) are represented in a fixed-size semantic vector in the same latent space, enabling direct comparison and integration. The training often involves techniques such as contrastive learning, where paired data points (e.g., an image and its associated caption) are brought closer in the embedding space, while non-matching pairs are pushed apart. This alignment captures both intra-modality (e.g., relationships within text or within images) and inter-modality (e.g., relationships between text and images) semantics.4. Structured Data EmbeddingsWhen dealing with structured data — that is, data organized in rows and columns — it is necessary to create a custom embedding model tailored to its specific application. In other words, in these cases, it is rare to find a pre-trained embedding model that can be directly used. We can find two common cases:General Structured Data: Embeddings for structured data like sensor data or tabular data can be generated using dimensionality reduction techniques like PCA. These embeddings can be used for anomaly detection and as inputs for downstream machine learning tasks.User/Item Structured Data: This type of structured data goes beyond a general data table. It includes multiple interconnected components: user data (e.g., demographics, preferences, or behaviors), item/product data (e.g., attributes like category, price, or brand), and data that describes the interaction between users and items (e.g., ratings, clicks, or purchase history).The primary goal of working with such data is to map both users and items into a shared embedding space, enabling recommender systems to predict interactions effectively. By representing users and items as fixed-size embeddings, the system can measure their similarity or relevance, making it possible to recommend the most suitable items for each user. One popular technique for this is Collaborative Filtering.5. Graph Embeddings:Graph embeddings are a way to represent nodes, edges, or entire graphs as fixed-size numerical vectors while preserving the structural and relational information inherent in the graph. These embeddings capture both the local (neighborhood) and global (overall structure) properties of the graph, enabling various downstream tasks like node classification, clustering, link prediction, recommendation systems, and more.Take an example of a social network where each person is a node, and the connections between people are defined as edges. Using graph embedding you can model each node as an embedding, such that the embedding captures not only the semantic information about the person itself, but also its relations and associations hence enriching the embedding. For example, if two nodes are connected by an edge, the vectors for those nodes would be similar. You might then be able to predict who the person is most similar to and recommend new connections.Popular algorithms for graph embedding include DeepWalk, Node2vec, LINE, and GraphSAGE.Training EmbeddingsModern embedding models often employ a dual encoder architecture, also known as a two-tower architecture, for training. In this setup, two separate neural networks, or “towers,” are used to encode different aspects of the data. For example, for the text embedding model used in question-answering, one tower is used to encode the queries and the other tower is used to encode the documents. For the image and text embedding model, one tower is used to encode the images and the other tower is used to encode the text.The training process typically involves a contrastive loss, a type of loss function that encourages the model to bring positive examples (semantically similar pairs) closer together in the embedding space while pushing negative examples (dissimilar pairs) further apart.Similar to LLMs training, the training of an embedding model from scratch includes two stages:Pre-training: In this unsupervised phase, the model learns general representations from a massive amount of unlabeled data.Fine-tuning: This supervised phase involves training the model on a smaller, labeled dataset specific to the downstream task. Fine-tuning helps adapt the model’s representations to the specific nuances of the target application.Remember, for unstructured data, pre-trained embeddings are typically available and can be directly used. But you may want to fine-tune them for domain-specific applications.Vector databasesOnce embeddings are created, the next challenge lies in efficiently storing and retrieving them. Historically, traditional databases lacked the means to combine semantic meaning and efficient querying in a way that the most relevant embeddings can be both stored, queried, and retrieved in a secure, scalable, and flexible manner. This is what gave rise to vector databases: specialized systems designed to manage and query embeddings at scale.Each vector database differs in its implementation, but the general flow is shown in the following figure:Press enter or click to view image in full sizeAn appropriate trained embedding model is used to embed the relevant data points as vectors with fixed dimensions.The vectors are then augmented with appropriate metadata and complementary information (such as tags) and indexed using the specified algorithm for efficient search.An incoming query gets embedded with the same model, and used to query and return specific amounts of the most semantically similar items and their associated unembedded content/metadata. Some databases might provide caching and pre-filtering (based on tags) and post-filtering capabilities (reranking using another more accurate model) to further enhance the query speed and performance.A few good examples of commercially managed vector databases areGoogle Vertex AI Vector SearchPineconeAnd some open-source examples are:WeaviateChromaVector searchVector search is the lifeblood of vector databases. It’s the mechanism that allows these databases to unlock the power of semantic similarity and deliver highly relevant results.Traditional vector search algorithms relied on keyword matching, which made them slow, difficult to scale, and incapable of returning results that did not exactly match the query’s keywords. In contrast, modern vector search techniques leverage embeddings to retrieve information based on semantic similarity, using metrics such as Euclidean distance, cosine similarity, or dot product. In other words, vector search methods identify the items in the database that are most similar to the given query based on their vector representations.Approximate Nearest Neighbor SearchOne of the most popular techniques for vector search is Approximate Nearest Neighbor (ANN) Search. ANN refers to a collection of algorithms designed to efficiently identify the closest (most similar) vectors in high-dimensional spaces.Why ANN?In vector databases (high-dimensional spaces), the computational cost of exact nearest neighbor (NN) searches grows exponentially with the size of the dataset and the number of dimensions. Instead, what ANN algorithms do is to trade off exact accuracy for speed and scalability, delivering results that are “good enough” in a fraction of the time.Popular ANN algorithms include:1. Hierarchical Navigable Small Worlds (HNSW)Strengths:- Extremely fast with high accuracy.- Scalable for large datasets.- Flexible in adapting to different similarity measures.Use Cases: General-purpose ANN tasks, recommendation systems, and high-dimensional vector retrieval.Tools: Integrated into libraries like FAISS and nmslib.2. Scalable Approximate Nearest Neighbor (ScaNN)Strengths:- Optimized for industrial-scale datasets.- Combines partitioning, approximate distance computation, and optional rescoring for top-tier speed-accuracy trade-offs.- Handles both dense and sparse vectors effectively.Use Cases: Google-scale applications, such as search and retrieval for embeddings.Tools: ScaNN.3. Annoy (Approximate Nearest Neighbors)Strengths:- Lightweight and simple to use.- Optimized for memory-mapped storage, making it great for static datasets.Use Cases: Recommendation systems, especially for smaller datasets or use cases where memory efficiency is key.Tools: Annoy.4. FAISS (Facebook AI Similarity Search)Strengths:- Comprehensive library supporting multiple ANN algorithms, including HNSW, IVFPQ (Inverted File Index with Product Quantization), and LSH.- Highly optimized for both CPU and GPU, making it ideal for large-scale searches.Use Cases: Large datasets with billions of vectors, real-time recommendations.Tools: FAISS.5. Locality Sensitive Hashing (LSH)Strengths:- Excellent for very high-dimensional sparse data.- Simple to implement and well-understood.Use Cases: Text search, genomic data, or any use case requiring radius-based similarity search.Tools: Available in libraries like scikit-learn and datasketch.Use case: Building a Knowledge-Intensive LLM with RAG and Vector DatabasesImagine you’re building a customer support chatbot that needs to provide accurate and up-to-date answers to customer queries. To achieve this, you can leverage a combination of LLMs, Retrieval Augmented Generation (RAG), and vector databases.Here’s a breakdown of the process:1. Data Preparation:Document Collection: Gather relevant documents like FAQs, product manuals, and customer support logs.Vectorization: Use a language model (e.g., BERT, RoBERTa) to convert each document into a dense vector representation. This vector captures the semantic meaning of the text.2. Vector Database:Indexing: Store these vectors in a vector database (e.g., Pinecone, Weaviate, Chroma). This database is optimized for efficient similarity search.3. RAG System:Query Processing: When a customer asks a question, convert it into a vector representation using the same language model.Retrieval: Query the vector database to find the most similar documents to the query vector.Augmentation: Combine the retrieved documents with the original query and feed them into the LLM.4. LLM Response Generation:The LLM processes the augmented prompt and generates a comprehensive and informative response.The response can be a direct answer to the query, a summary of relevant information, or a combination of both.ConclusionEmbeddings have revolutionized the way we work with diverse data in machine learning. Their ability to capture semantic relationships, facilitate efficient comparisons, and serve as inputs for various models has opened up a world of possibilities. By choosing the right embedding technique, vector database, and search algorithm, developers can unlock the full potential of embeddings to build powerful applications.Key use cases, such as semantic search, recommendation systems, anomaly detection, and LLMs with RAG, showcase the versatility of embeddings in addressing complex challenges. As the technology continues to evolve, embeddings will remain a fundamental building block for advancing AI-driven solutions.If you’re interested in related content, I recommend reading my next article, Introduction to AI Agents. It explores the world of AI agents, covering its architecture, core components, and implementation for real-world applications.Introduction to AI AgentsArchitecture, Tools, and Implementationmedium.comReferences[1] Nawalgaria, A., & Ren, X. (2024). Embeddings & Vector Stores.Press enter or click to view image in full sizeAnd that’s it! Thank you for reading! I hope you enjoyed this article. If you’d like, add me on LinkedIn.EmbeddingVector DatabaseVector SearchLlmAI----1Written by Aleix López Pascual124 followers·6 followingSenior Data Scientist @ Glovo | Competitions Expert @ Kaggle | Writer @ Medium | MSc in High Energy Physics, Astrophysics and CosmologyResponses (1)See all responsesHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://medium.com/@aleixlopez/introduction-to-prompt-engineering-16fb3cbfb031', 'title': 'Introduction to Prompt Engineering | by Aleix López Pascual | Medium', 'description': 'Learn the fundamentals of prompt engineering and unlock the full potential of Large Language Models (LLMs). Discover essential techniques, best practices, and practical examples to master this critical AI skill.', 'language': 'en'}, page_content='Introduction to Prompt Engineering | by Aleix López Pascual | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inIntroduction to Prompt EngineeringMastering the Art of Prompt Engineering: Techniques, Best Practices, and Real-World ApplicationsAleix López Pascual10 min read·Nov 23, 2024--ListenShareThis article explores the fascinating world of prompt engineering, a critical skill for effectively leveraging the power of Large Language Models (LLMs) in a variety of applications. We’ll cover essential techniques, best practices, and practical examples to help you become a proficient prompt engineer.For a quick refresher on the basics of LLMs, consider visiting this article:Introduction to Foundational Large Language ModelsThe underlying architecture, training methodologies, fine-tuning techniques, and inference optimization methods for…medium.comUnderstanding Prompts and Prompt EngineeringAt its core, a prompt is the input you provide to an LLM to elicit a desired response. It could be a question, a statement, instructions, or even a combination of text and other modalities like images. Prompt engineering, then, is the art and science of crafting effective prompts to guide LLMs towards generating accurate and insightful outputs.It’s an iterative process involving experimentation, optimisation, and careful consideration of various factors that influence LLM behaviour.Controlling LLM Output: Configuration is KeyLLMs offer a range of configuration settings that significantly impact the generated text. Understanding these settings is crucial for tailoring the output to your specific needs.Output Length: Determines the maximum number of tokens the LLM will generate. Generating more tokens requires more computation from the LLM, leading to higher energy consumption, potentially slower response times, and higher costs.Temperature: Controls the randomness of the LLM’s output. Lower temperature values (e.g., 0.1) make the responses more predictable and deterministic, returning the tokens with highest probability. Higher temperatures (e.g., 0.9) introduce more randomness, allowing for more diverse and creative results.Top-K Sampling: Imagine the LLM has generated a probability distribution for the next token in a sequence. Top-K sampling restricts the LLM to consider only the K most likely tokens from this distribution. For instance, if K = 5, the LLM will choose the next token randomly from only the top 5 most likely candidates.Top-P Sampling (Nucleus Sampling): This method takes a slightly different approach. Instead of fixing the number of tokens to consider, it sets a probability threshold P. The LLM then selects tokens from the probability distribution, starting with the most likely, until the cumulative probability of the selected tokens reaches or exceeds P. For example, if P = 0.9, the LLM might choose the top 3 most likely tokens if their combined probability is 0.92, even though other less likely tokens exist.Examples:Press enter or click to view image in full sizeIt is recommended starting with a temperature of 0.2, top-P of 0.95, and top-K of 30 for generally coherent and creative results.Prompting TechniquesLet’s delve into a variety of prompting techniques that enable you to interact with LLMs more effectively and achieve your desired results.Zero-Shot Prompting:This is the most basic approach, where you provide a task description and some input text without any examples. It relies on the LLM’s inherent knowledge and ability to generalize from its training data.Press enter or click to view image in full sizeOne-Shot and Few-Shot Prompting:These techniques involve providing one or more examples within the prompt to guide the LLM towards a specific output structure or pattern. The idea is to show the model what you expect, making it particularly helpful for tasks like code generation, translation, or data extraction.Press enter or click to view image in full sizeSystem prompting:Sets the overall context and purpose for the language model. It defines the ‘big picture’ of the model’s intended task, such as language translation, review classification, etc. It also used to return a certain structure or format, for instance JSON format or uppercase.Press enter or click to view image in full sizeContextual prompting:Provides specific details or background information relevant to the current conversation or task. It helps the model comprehend the nuances of the query and tailor the response accordingly.Press enter or click to view image in full sizeRole prompting:Asks the LLM to adopt a specific persona, influencing the tone and style of its output. This helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior.Press enter or click to view image in full sizeStep-Back Prompting:This technique encourages the LLM to think more critically by first prompting it to first consider a general question related to the specific task at hand, and then feeding the answer to that general question into a subsequent prompt for the specific task. This “step back” helps activate relevant background knowledge and can lead to more insightful and accurate responses.Prompt 1:Press enter or click to view image in full sizePrompt 2:Press enter or click to view image in full sizeChain of Thought (CoT) Prompting:CoT enhances the LLM’s reasoning abilities by explicitly prompting it to generate intermediate reasoning steps. This makes the reasoning process transparent, improves accuracy, and can be particularly helpful for complex tasks involving logical deduction or problem-solving.Press enter or click to view image in full sizeSelf-Consistency Prompting:This technique leverages the power of multiple reasoning paths to improve accuracy. It works by sending the same CoT prompt multiple times with a high-temperature setting, encouraging diverse reasoning, and then selecting the most frequent answer. This offers a sort of “wisdom of the crowd” approach to LLM reasoning.Tree of Thoughts (ToT) Prompting:ToT further generalizes CoT prompting by allowing the LLM to explore multiple reasoning paths concurrently, like branches on a tree. Unlike CoT, which follows a single linear chain of thought, ToT enables exploration of various possibilities, making it particularly effective for complex tasks that require exploring various possibilities before arriving at a solution.Implementation: ToT can be implemented manually. This involves crafting specific prompts that guide the LLM to explore multiple reasoning paths. Then, continue prompting in the most promising direction to continue exploring. If a prompt is unproductive, we backtrack to a previous prompt and explore alternative paths.Alternatively, it can be implemented automatically using a Python script or libraries like LangChain.ReAct (Reason & Act) Prompting:ReAct is a popular technique that enhances the capabilities of LLMs by enabling them to interact with external tools and APIs. This interaction allows the LLM to retrieve information, perform computations, and execute actions to solve complex problems that would be difficult or impossible with text-based input alone.Here’s a breakdown of how ReAct prompting works:Thought-Action Loop: ReAct prompting operates through a continuous loop of reasoning and acting. The LLM starts by analysing the given prompt and formulates a plan of action. This plan may involve utilising external tools to gather additional information.Reasoning and Planning: The LLM uses its internal knowledge and the information gathered from external sources to reason about the problem. It breaks down the problem into smaller steps and devises a strategy to achieve the desired outcome.Action Execution: The LLM then interacts with the appropriate external tools, such as search engines, APIs, or databases, to execute the planned actions. This could involve searching for relevant information, retrieving specific data points, or performing calculations.Observation and Feedback: The LLM observes the results of its actions and incorporates the new information into its reasoning process. This feedback loop allows the LLM to adapt its plan and refine its approach as it progresses towards the solution.Example:Imagine you want to know the total number of children fathered by members of the band Metallica. A ReAct prompt would instruct the LLM to:Identify the members of Metallica.For each member, search for information about their children.Sum the number of children across all band members.The LLM would then interact with a search engine API, retrieving information about each band member and their offspring. By combining reasoning and external tool usage, the LLM can provide an accurate answer to this question.Implementation:Implementing ReAct prompting typically involves using libraries like LangChain, which provides tools for integrating LLMs with external APIs and resources. Developers can then create agents that leverage ReAct prompting to solve specific problems.Automatic Prompt EngineeringIn the previous section we explored various manual prompting techniques, which required careful crafting and refinement by humans. Wouldn’t it be nice to automate this (write a prompt to write prompts)? Well, there’s a method: Automatic Prompt Engineering (APE).The core idea behind APE is to use an LLM to generate a variety of prompt candidates, evaluate these candidates based on a chosen metric, and then select the best-performing prompt. This iterative process can be repeated to further refine the prompt and optimise its effectiveness.Here is a breakdown of the steps involved in APE:Prompt Generation: An initial prompt is given to the LLM with instructions to generate multiple variations of the prompt, while maintaining the same semantic meaning. For instance, if the goal is to train a chatbot for a merchandise t-shirt webshop, the initial prompt could ask the LLM to generate different ways a customer might order a specific t-shirt.Prompt Evaluation: Each of the generated prompt candidates is then evaluated using a suitable metric. Common metrics used for evaluating prompts include BLEU (Bilingual Evaluation Understudy) and ROUGE (Recall-Oriented Understudy for Gisting Evaluation). These metrics assess the quality and relevance of the generated prompts based on various criteria, such as fluency, coherence, and semantic similarity to the original prompt.Prompt Selection: The prompt candidate with the highest evaluation score is selected as the final prompt for the intended application. This selected prompt can be further tweaked and re-evaluated if needed.Example:Let’s consider the t-shirt webshop chatbot again. You provide an initial prompt to an LLM, such as: “We have a band merchandise t-shirt webshop, and to train a chatbot, we need various ways to order: ‘One Metallica t-shirt size S’. Generate 10 variants with the same semantics but keep the same meaning”. The LLM would then generate a list of alternative ways to phrase this order.Best Practices for Effective Prompt EngineeringTo truly master the art of prompt engineering, here are some best practices distilled from the sources:Provide Examples: Whenever possible, use one-shot or few-shot learning to demonstrate the desired output structure or pattern. Examples act as powerful teaching tools for LLMs.Design with Simplicity: Keep your prompts concise, clear, and easy to understand. Avoid complex language and unnecessary information. Use verbs that clearly describe the desired action (e.g., summarise, explain, generate).Be Specific About the Output: Don’t be vague! Provide explicit instructions on the desired format, style, length, and content of the response.Prioritise Instructions Over Constraints: Focus on telling the LLM what to do, rather than what not to do. Use constraints only when necessary for safety, clarity, or specific requirements.Control Max Token Length: Set limits either in the configuration or explicitly in the prompt to manage output length and cost.Use Variables in Prompts: Enhance reusability and flexibility by incorporating variables that can be easily changed for different inputs. This is particularly useful for building applications that interact with LLMs.Experiment and Iterate: The key to success in prompt engineering is experimentation. Try different input formats, writing styles, model configurations, and even collaborate with other prompt engineers to compare approaches and discover what works best for your specific tasks.Adapt to Model Updates: LLMs are constantly evolving. Stay informed about updates to model architecture, training data, and capabilities. Revisit and refine your prompts to leverage new features effectively.Explore Output Formats: Consider requesting structured output formats like JSON for tasks involving data extraction, organisation, or analysis. This can streamline downstream processing and reduce the need for manual formatting.Document Your Experiments: Maintain a detailed record of your prompt attempts, model configurations, outputs, and observations. This will help you track progress, identify patterns, and refine your prompting strategies over time.ConclusionPrompt engineering is a journey of continuous exploration and refinement. The techniques and best practices discussed in this article provide a solid foundation for you to start experimenting, iterating, and ultimately mastering the art of communicating effectively with LLMs. Embrace the iterative nature of prompt engineering, document your findings, and continue to push the boundaries of what’s possible with these powerful language models. As the field of LLMs advances, so too will the importance of prompt engineering in unlocking their full potential.If you’re interested in related content, I recommend reading my next article, Introduction to Embeddings & Vector Stores. It explores the role of embeddings in machine learning, how to create them, how to manage manage them at scale, and real-world applications.Introduction to Embeddings & Vector StoresThe role of embeddings in machine learning, how to create them, how to manage manage them at scale, and real-world…medium.comReferences[1] Boonstra, L. (2024). Prompt Engineering.Press enter or click to view image in full sizeAnd that’s it! Thank you for reading! I hope you enjoyed this article. If you’d like, add me on LinkedIn.Prompt EngineeringLarge Language ModelsArtificial IntelligenceData ScienceMachine Learning----Written by Aleix López Pascual124 followers·6 followingSenior Data Scientist @ Glovo | Competitions Expert @ Kaggle | Writer @ Medium | MSc in High Energy Physics, Astrophysics and CosmologyNo responses yetHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00d3b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7771201",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool_AIAgent = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search and run information about AI Agents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5931acf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_db_blog', description='Search and run information about AI Agents', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x119ff3060>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11b9c27b0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x119ff31a0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11b9c27b0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool_AIAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a3024cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_urls = [\n",
    "    \"https://medium.com/@nimritakoul01/the-model-context-protocol-mcp-a-complete-tutorial-a3abe8a7f4ef\",\n",
    "    \"https://medium.com/@laurentkubaski/mcp-prompts-explained-including-how-to-actually-use-them-9db13d69d7e2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb258faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdd6f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool_MCP = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search and run information about MCP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3cd83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
